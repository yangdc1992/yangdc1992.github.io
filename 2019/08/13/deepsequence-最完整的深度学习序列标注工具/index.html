<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  

  
  <title>deepsequence-最完整的深度学习序列标注工具 | DcYang</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="简介deepsequence 是我在工作学习过程中写的一个做序列标注的工具，基于keras,支持各种经典的序列标注任务，如分词，NER(命名实体识别), 为什么是最完整的呢？  支持BERT+CRF 和 BILSTM+CRF两种最优秀的序列标注的算法架构 支持字符级别的特征，charCNN 或者 charLSTM 支持postag特征的输入 支持领域知识的输入，如用于ner识别的公司字典 支持BI">
<meta name="keywords" content="nlp, machine learning, deep learning">
<meta property="og:type" content="article">
<meta property="og:title" content="deepsequence-最完整的深度学习序列标注工具">
<meta property="og:url" content="http://yoursite.com/2019/08/13/deepsequence-最完整的深度学习序列标注工具/index.html">
<meta property="og:site_name" content="DcYang">
<meta property="og:description" content="简介deepsequence 是我在工作学习过程中写的一个做序列标注的工具，基于keras,支持各种经典的序列标注任务，如分词，NER(命名实体识别), 为什么是最完整的呢？  支持BERT+CRF 和 BILSTM+CRF两种最优秀的序列标注的算法架构 支持字符级别的特征，charCNN 或者 charLSTM 支持postag特征的输入 支持领域知识的输入，如用于ner识别的公司字典 支持BI">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2019-08-13T10:26:33.183Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="deepsequence-最完整的深度学习序列标注工具">
<meta name="twitter:description" content="简介deepsequence 是我在工作学习过程中写的一个做序列标注的工具，基于keras,支持各种经典的序列标注任务，如分词，NER(命名实体识别), 为什么是最完整的呢？  支持BERT+CRF 和 BILSTM+CRF两种最优秀的序列标注的算法架构 支持字符级别的特征，charCNN 或者 charLSTM 支持postag特征的输入 支持领域知识的输入，如用于ner识别的公司字典 支持BI">
  
    <link rel="alternate" href="/atom.xml" title="DcYang" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">DcYang</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">dcyang&#39;s personal website</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-deepsequence-最完整的深度学习序列标注工具" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/13/deepsequence-最完整的深度学习序列标注工具/" class="article-date">
  <time datetime="2019-08-13T09:55:53.000Z" itemprop="datePublished">2019-08-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      deepsequence-最完整的深度学习序列标注工具
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>deepsequence 是我在工作学习过程中写的一个做序列标注的工具，基于keras,支持各种经典的序列标注任务，如分词，NER(命名实体识别), 为什么是最完整的呢？</p>
<ul>
<li>支持BERT+CRF 和 BILSTM+CRF两种最优秀的序列标注的算法架构</li>
<li>支持字符级别的特征，charCNN 或者 charLSTM</li>
<li>支持postag特征的输入</li>
<li>支持领域知识的输入，如用于ner识别的公司字典</li>
<li>支持BIO, BIOLU, BIOES三种标注格式</li>
<li>支持通过配置文件快速定制模型细节及参数</li>
</ul>
<h2 id="架构解析"><a href="#架构解析" class="headerlink" title="架构解析"></a>架构解析</h2><h3 id="BILSTM-CRF"><a href="#BILSTM-CRF" class="headerlink" title="BILSTM + CRF"></a>BILSTM + CRF</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line">def build_bilstm(self, verbose=True):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    build model architecture from parameters</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    word_ids = Input(batch_shape=(None, None), dtype=&apos;int32&apos;, name=&apos;word_input&apos;)</span><br><span class="line">    inputs = [word_ids]</span><br><span class="line"></span><br><span class="line">    if self._params.use_pretrain_embedding:</span><br><span class="line">        if verbose: logging.info(&quot;initial word embedding with pretrained embeddings&quot;)</span><br><span class="line">        if self._params.word_embedding_dim == 100:</span><br><span class="line">            glove_file = self._params.data_dir + &apos;/glove.6B.100d.txt&apos;</span><br><span class="line">        elif self._params.word_embedding_dim == 300:</span><br><span class="line">            glove_file = self._params.data_dir + &apos;/glove.42B.300d.txt&apos;</span><br><span class="line">        else:</span><br><span class="line">            logging.error(&quot;we only support glove embedding with dimension 100 or 300&quot;)</span><br><span class="line">            raise ValueError(&quot;unmatch word dimension, we only support glove embedding with dimension 100 or 300&quot;)</span><br><span class="line">        glove_embedding_index = load_glove(glove_file, self._params.word_embedding_dim)</span><br><span class="line">        word_vocab = self.input_processor.word_vocab.vocab</span><br><span class="line">        glove_embeddings_matrix = np.zeros([len(word_vocab), self._params.word_embedding_dim])</span><br><span class="line">        for word, i in word_vocab.items():</span><br><span class="line">            vector = glove_embedding_index.get(word)</span><br><span class="line">            if vector is not None:</span><br><span class="line">                glove_embeddings_matrix[i] = vector</span><br><span class="line">        </span><br><span class="line">        word_embeddings = Embedding(input_dim=glove_embeddings_matrix.shape[0],</span><br><span class="line">                                    output_dim=glove_embeddings_matrix.shape[1],</span><br><span class="line">                                    trainable=False,</span><br><span class="line">                                    mask_zero=True,</span><br><span class="line">                                    weights=[glove_embeddings_matrix],</span><br><span class="line">                                    name=&apos;word_embedding&apos;)(word_ids)</span><br><span class="line">    else:</span><br><span class="line">        word_embeddings = Embedding(input_dim=self._params.word_vocab_size,</span><br><span class="line">                                    output_dim=self._params.word_embedding_dim,</span><br><span class="line">                                    mask_zero=True,</span><br><span class="line">                                    name=&apos;word_embedding&apos;)(word_ids)</span><br><span class="line"></span><br><span class="line">    input_embeddings = [word_embeddings]</span><br><span class="line">    if self._params.use_char:</span><br><span class="line">        char_ids = Input(batch_shape=(None, None, None), dtype=&apos;int32&apos;, name=&apos;char_input&apos;)</span><br><span class="line">        inputs.append(char_ids)</span><br><span class="line">        if self._params.char_feature == &quot;lstm&quot;:</span><br><span class="line">            char_embeddings = Embedding(input_dim=self._params.char_vocab_size,</span><br><span class="line">                                        output_dim=self._params.char_embedding_dim,</span><br><span class="line">                                        mask_zero=True,</span><br><span class="line">                                        name=&apos;char_embedding&apos;)(char_ids)</span><br><span class="line">            if verbose: logging.info(&quot;using charcter level lstm features&quot;)</span><br><span class="line">            char_feas = TimeDistributed(Bidirectional(LSTM(self._params.char_lstm_size)), name=&quot;char_lstm&quot;)(char_embeddings)</span><br><span class="line">        elif self._params.char_feature == &quot;cnn&quot;:</span><br><span class="line">            # cnn do not support mask</span><br><span class="line">            char_embeddings = Embedding(input_dim=self._params.char_vocab_size,</span><br><span class="line">                                        output_dim=self._params.char_embedding_dim,</span><br><span class="line">                                        name=&apos;char_embedding&apos;)(char_ids)</span><br><span class="line">            if verbose: logging.info(&quot;using charcter level cnn features&quot;)</span><br><span class="line">            char_feas = char_cnn_encode(char_embeddings, self._params.n_gram_filter_sizes, self._params.n_gram_filter_nums)</span><br><span class="line">        else:</span><br><span class="line">            raise ValueError(&apos;char feature must be lstm or cnn&apos;)</span><br><span class="line"></span><br><span class="line">        input_embeddings.append(char_feas)</span><br><span class="line"></span><br><span class="line">    if self._params.use_pos:</span><br><span class="line">        if verbose: logging.info(&quot;use pos tag features&quot;)</span><br><span class="line">        pos_ids = Input(batch_shape=(None, None), dtype=&apos;int32&apos;, name=&apos;pos_input&apos;)</span><br><span class="line">        inputs.append(pos_ids)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        pos_embeddings = Embedding(input_dim=self._params.pos_vocab_size,</span><br><span class="line">                                    output_dim=self._params.pos_embedding_dim,</span><br><span class="line">                                    mask_zero=True,</span><br><span class="line">                                    name=&apos;pos_embedding&apos;)(pos_ids)</span><br><span class="line">        input_embeddings.append(pos_embeddings)</span><br><span class="line"></span><br><span class="line">    if self._params.use_dict:</span><br><span class="line">        if verbose: logging.info(&quot;use user dict features&quot;)</span><br><span class="line">        dict_ids = Input(batch_shape=(None, None), dtype=&apos;int32&apos;, name=&apos;dict_input&apos;)</span><br><span class="line">        inputs.append(dict_ids)</span><br><span class="line"></span><br><span class="line">        dict_embeddings = Embedding(input_dim=self._params.dict_vocab_size,</span><br><span class="line">                                    output_dim=self._params.dict_embedding_dim,</span><br><span class="line">                                    mask_zero=True,</span><br><span class="line">                                    name=&apos;dict_embedding&apos;)(dict_ids)</span><br><span class="line">        input_embeddings.append(dict_embeddings)</span><br><span class="line"></span><br><span class="line">    input_embedding = Concatenate(name=&quot;input_embedding&quot;)(input_embeddings) if len(input_embeddings)&gt;1 else input_embeddings[0]</span><br><span class="line">    input_embedding_ln = LayerNormalization(name=&apos;input_layer_normalization&apos;)(input_embedding)</span><br><span class="line">    #input_embedding_bn = BatchNormalization()(input_embedding_ln)</span><br><span class="line">    input_embedding_drop = Dropout(self._params.dropout, name=&quot;input_embedding_dropout&quot;)(input_embedding_ln)</span><br><span class="line"></span><br><span class="line">    z = Bidirectional(LSTM(units=self._params.main_lstm_size, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),</span><br><span class="line">                       name=&quot;main_bilstm&quot;)(input_embedding_drop)</span><br><span class="line">    z = Dense(self._params.fc_dim, activation=&apos;tanh&apos;, name=&quot;fc_dense&quot;)(z)</span><br><span class="line"></span><br><span class="line">    if self._params.use_crf:</span><br><span class="line">        if verbose: logging.info(&apos;use crf decode layer&apos;)</span><br><span class="line">        crf = CRF(self._params.num_labels, sparse_target=False,</span><br><span class="line">                    learn_mode=&apos;marginal&apos;, test_mode=&apos;marginal&apos;, name=&apos;crf_out&apos;)</span><br><span class="line">        loss = crf.loss_function</span><br><span class="line">        pred = crf(z)</span><br><span class="line">    else:</span><br><span class="line">        loss = &apos;categorical_crossentropy&apos;</span><br><span class="line">        pred = Dense(self._params.num_labels, activation=&apos;softmax&apos;, name=&apos;softmax_out&apos;)(z)</span><br><span class="line"></span><br><span class="line">    model = Model(inputs=inputs, outputs=pred)</span><br><span class="line">    model.summary(print_fn=lambda x: logging.info(x + &apos;\n&apos;))</span><br><span class="line">    model.compile(loss=loss, optimizer=self._params.optimizer)</span><br><span class="line"></span><br><span class="line">    self.model = model</span><br></pre></td></tr></table></figure>

<h3 id="BERT-CRF"><a href="#BERT-CRF" class="headerlink" title="BERT + CRF"></a>BERT + CRF</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">def build_bert(self, verbose=True):</span><br><span class="line"></span><br><span class="line">    bert_word_ids = Input(batch_shape=(None, None), dtype=&quot;int32&quot;, name=&quot;bert_word_input&quot;)</span><br><span class="line">    bert_mask_ids = Input(batch_shape=(None, None), dtype=&quot;int32&quot;, name=&apos;bert_mask_input&apos;)</span><br><span class="line">    bert_segment_ids = Input(batch_shape=(None, None), dtype=&quot;int32&quot;, name=&quot;bert_segment_input&quot;)</span><br><span class="line">    </span><br><span class="line">    inputs = [bert_word_ids, bert_mask_ids, bert_segment_ids]</span><br><span class="line"></span><br><span class="line">    bert_out = BertLayer(n_fine_tune_layers=self._params.n_fine_tune_layers, bert_path=self._params.bert_path, name=&quot;bert_layer&quot;)([bert_word_ids, bert_mask_ids, bert_segment_ids])</span><br><span class="line"></span><br><span class="line">    features = bert_out</span><br><span class="line"></span><br><span class="line">    if self._params.use_dict:</span><br><span class="line">        if verbose: logging.info(&quot;use user dict features&quot;)</span><br><span class="line">        dict_ids = Input(batch_shape=(None, None), dtype=&apos;int32&apos;, name=&apos;dict_input&apos;)</span><br><span class="line">        inputs.append(dict_ids)</span><br><span class="line"></span><br><span class="line">        dict_embeddings = Embedding(input_dim=self._params.dict_vocab_size,</span><br><span class="line">                                    output_dim=self._params.dict_embedding_dim,</span><br><span class="line">                                    mask_zero=True,</span><br><span class="line">                                    name=&apos;dict_embedding&apos;)(dict_ids)</span><br><span class="line"></span><br><span class="line">        features = Concatenate(name=&quot;bert_and_dict_features&quot;)([features, dict_embeddings])</span><br><span class="line"></span><br><span class="line">    z = Dense(self._params.fc_dim, activation=&apos;relu&apos;, name=&quot;fc_dense&quot;)(features)</span><br><span class="line"></span><br><span class="line">    if self._params.use_crf:</span><br><span class="line">        if verbose: logging.info(&apos;use crf decode layer&apos;)</span><br><span class="line">        crf = CRF(self._params.num_labels, sparse_target=False,</span><br><span class="line">                    learn_mode=&apos;marginal&apos;, test_mode=&apos;marginal&apos;, name=&apos;crf_out&apos;)</span><br><span class="line">        loss = crf.loss_function</span><br><span class="line">        pred = crf(z)</span><br><span class="line">    else:</span><br><span class="line">        loss = &apos;categorical_crossentropy&apos;</span><br><span class="line">        pred = Dense(self._params.num_labels, activation=&apos;softmax&apos;, name=&apos;softmax_out&apos;)(z)</span><br><span class="line"></span><br><span class="line">    model = Model(inputs=inputs, outputs=pred)</span><br><span class="line">    model.summary(print_fn=lambda x: logging.info(x + &apos;\n&apos;))</span><br><span class="line"></span><br><span class="line">    # It is recommended that you use this optimizer for fine tuning, since this</span><br><span class="line">    # is how the model was trained (note that the Adam m/v variables are NOT</span><br><span class="line">    # loaded from init_checkpoint.)</span><br><span class="line">    optimizer = AdamWeightDecayOptimizer(</span><br><span class="line">        learning_rate=1e-5,</span><br><span class="line">        weight_decay_rate=0.01,</span><br><span class="line">        beta_1=0.9,</span><br><span class="line">        beta_2=0.999,</span><br><span class="line">        epsilon=1e-6,</span><br><span class="line">        exclude_from_weight_decay=[&quot;LayerNorm&quot;, &quot;layer_norm&quot;, &quot;bias&quot;])</span><br><span class="line">    </span><br><span class="line">    model.compile(loss=loss, optimizer=optimizer)</span><br><span class="line"></span><br><span class="line">    self.model = model</span><br></pre></td></tr></table></figure>

<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>训练NER模型：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">import argparse</span><br><span class="line">import logging</span><br><span class="line"></span><br><span class="line">from deepsequence.dataset import load_conll</span><br><span class="line">from deepsequence.model import SequenceModel</span><br><span class="line">from deepsequence.config import Params</span><br><span class="line">from deepsequence.utils import set_logger</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main(params):</span><br><span class="line"></span><br><span class="line">    train_file = params.data_dir + &apos;/train.txt&apos;</span><br><span class="line">    valid_file = params.data_dir + &apos;/valid.txt&apos;</span><br><span class="line">    train_data = load_conll(train_file, params)</span><br><span class="line">    valid_data = load_conll(valid_file, params)</span><br><span class="line"></span><br><span class="line">    model = SequenceModel(params)</span><br><span class="line"></span><br><span class="line">    model_file = params.data_dir + &apos;/model/model.h5&apos;</span><br><span class="line">    if params.continue_previous_training is True:</span><br><span class="line"></span><br><span class="line">        logging.info(&quot;restore model from local&quot;)</span><br><span class="line">        model.restore(model_file)</span><br><span class="line">    else:</span><br><span class="line">        logging.info(&quot;model initializing...&quot;)</span><br><span class="line">        model.build(params)</span><br><span class="line"></span><br><span class="line">    model.fit(train_data, valid_data, verbose=True)</span><br><span class="line">    model.evaluate(valid_data)</span><br><span class="line">    logging.info(&quot;model save to &#123;&#125;&quot;.format(model_file))</span><br><span class="line">    model.save(model_file)</span><br><span class="line"></span><br><span class="line">    tf_saved_model_dir = params.data_dir + &apos;/model/tf_saved_model&apos;</span><br><span class="line">    model.export_sm(tf_saved_model_dir)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line"></span><br><span class="line">    parser = argparse.ArgumentParser(description=&apos;&apos;&apos;train deep sequence model&apos;&apos;&apos;)</span><br><span class="line">    parser.add_argument(&apos;--config&apos;, required=True)</span><br><span class="line"></span><br><span class="line">    set_logger(&apos;train.log&apos;)</span><br><span class="line"></span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    logging.info(&quot;parse config file path: &#123;&#125;&quot;.format(args.config))</span><br><span class="line"></span><br><span class="line">    params = Params(args.config)</span><br><span class="line">    logging.info(&quot;parameters: &#123;&#125;&quot;.format(params.dict))</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        main(params)</span><br><span class="line">    except Exception as e:</span><br><span class="line">        logging.error(&apos;run fail&apos;, exc_info=True)</span><br></pre></td></tr></table></figure>

<p>配置文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;model&quot;: &quot;bilstm&quot;,</span><br><span class="line">    </span><br><span class="line">    &quot;vocab_pad&quot;: &quot;PAD&quot;,</span><br><span class="line">    &quot;vocab_unk&quot;: &quot;UNK&quot;,</span><br><span class="line">    &quot;max_sent_len&quot;: 1000,</span><br><span class="line">    &quot;max_word_len&quot;: 24,</span><br><span class="line"></span><br><span class="line">    &quot;word_embedding_dim&quot;: 100,</span><br><span class="line">    &quot;use_pretrain_embedding&quot;: true, </span><br><span class="line"></span><br><span class="line">    &quot;use_char&quot;: true,</span><br><span class="line">    &quot;char_embedding_dim&quot;: 30, </span><br><span class="line">    &quot;char_feature&quot;: &quot;cnn&quot;,</span><br><span class="line">    &quot;n_gram_filter_sizes&quot;: [1, 2, 3, 4, 5],</span><br><span class="line">    &quot;n_gram_filter_nums&quot;: [5, 10, 20, 30, 35],</span><br><span class="line">    &quot;char_lstm_size&quot;: 50,</span><br><span class="line"></span><br><span class="line">    &quot;use_pos&quot;: true, </span><br><span class="line">    &quot;conll_pos_index&quot;: 1,</span><br><span class="line">    &quot;pos_embedding_dim&quot;: 30,</span><br><span class="line"></span><br><span class="line">    &quot;use_dict&quot;: true, </span><br><span class="line">    &quot;dict_embedding_dim&quot;: 10,</span><br><span class="line">     </span><br><span class="line">    &quot;dropout&quot;: 0.5, </span><br><span class="line">     </span><br><span class="line">    &quot;main_lstm_size&quot;: 100, </span><br><span class="line">    &quot;fc_dim&quot;: 100, </span><br><span class="line">    &quot;use_crf&quot;: true, </span><br><span class="line">    &quot;optimizer&quot;: &quot;adam&quot;, </span><br><span class="line"></span><br><span class="line">    &quot;batch_size&quot;: 50, </span><br><span class="line">    &quot;max_train_epoch&quot;: 80, </span><br><span class="line">    </span><br><span class="line">    &quot;early_stop&quot;: 80, </span><br><span class="line">    </span><br><span class="line">    &quot;continue_previous_training&quot;: false,</span><br><span class="line">    &quot;data_dir&quot;: &quot;/xxxx/deepsequence/examples/data&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>更多细节可以可以在项目的GitHub页面查看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/yangdc1992/deepsequence</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/13/deepsequence-最完整的深度学习序列标注工具/" data-id="cjzsj8lxj00012l4uwpnuxd3u" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/08/26/短文本的几点思考/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          短文本的几点思考
        
      </div>
    </a>
  
  
    <a href="/2019/08/13/主动学习/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">主动学习</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/08/26/序列标注进化史/">序列标注进化史</a>
          </li>
        
          <li>
            <a href="/2019/08/26/短文本的几点思考/">短文本的几点思考</a>
          </li>
        
          <li>
            <a href="/2019/08/13/deepsequence-最完整的深度学习序列标注工具/">deepsequence-最完整的深度学习序列标注工具</a>
          </li>
        
          <li>
            <a href="/2019/08/13/主动学习/">主动学习</a>
          </li>
        
          <li>
            <a href="/2019/08/13/小数据VS大模型/">小数据VS大模型</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 杨德城/Decheng Yang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>