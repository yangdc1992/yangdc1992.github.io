<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  

  
  <title>DcYang</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="keep learning! deep learner">
<meta name="keywords" content="nlp, machine learning, deep learning">
<meta property="og:type" content="website">
<meta property="og:title" content="DcYang">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="DcYang">
<meta property="og:description" content="keep learning! deep learner">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DcYang">
<meta name="twitter:description" content="keep learning! deep learner">
  
    <link rel="alternate" href="/atom.xml" title="DcYang" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">DcYang</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">dcyang&#39;s personal website</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-序列标注进化史" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/26/序列标注进化史/" class="article-date">
  <time datetime="2019-08-26T14:36:25.000Z" itemprop="datePublished">2019-08-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/26/序列标注进化史/">序列标注进化史</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="HMM-gt-MEMM-gt-CRF-gt-RNN-gt-LSTM-gt-Attention"><a href="#HMM-gt-MEMM-gt-CRF-gt-RNN-gt-LSTM-gt-Attention" class="headerlink" title="HMM -&gt; MEMM -&gt; CRF -&gt; RNN -&gt; LSTM -&gt; Attention"></a>HMM -&gt; MEMM -&gt; CRF -&gt; RNN -&gt; LSTM -&gt; Attention</h2><p>序列标注算法主体上经历了从隐马尔可夫，最大熵马尔可夫， 条件随机场， RNN, LSTM 到 transformer的一个演进过程，这是一个递进发展的过程。</p>
<h3 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h3><ul>
<li><p>两个假设<br>  状态独立假设： 当前状态只依赖于上一个状态<br>  观测独立假设： 当前观测变量只依赖于对应的状态变量</p>
</li>
<li><p>两个问题<br>  标签偏置问题： 即算法倾向于选择分支较少的状态，这是由于状态独立假设使得在计算标签转移概率时在局部归一化状态转移概率<br>  特征缺失： 由于观测独立假设，HMM无法将上下文信息融入，所以会有较大的局限性</p>
</li>
</ul>
<h3 id="MEMM"><a href="#MEMM" class="headerlink" title="MEMM"></a>MEMM</h3><p>MEMM 取消了HMM的观测独立假设，它是判别模型的一种，引入了特征，可以方便的把上下文信息设计到特征中，解决了上下文信息缺失的问题，但MEMM没有取消HMM的状态独立假设，所以依然存在标签偏置的问题</p>
<h3 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h3><p>CRF 在MEMM的基础上进一步取消了HMM的状态独立假设，将标签转移也作为全局特征之一，在全局进行优化，所以crf解决了标签偏置以及上下文信息缺失的问题</p>
<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>RNN是经典的深度学习序列模型，相比于CRF, RNN在词向量的基础上做特征抽象，比CRF的特征模板的方式能够更好的避免过拟合，此外，RNN在序列上下文特征上相比crf会更加有效</p>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>LSTM是RNN的改进模型之一， LSTM加入了门控单元，使得门控单元的阈值可以随上下文以及当前输入动态改变，从而改善RNN由于梯度传播过程中参数连乘造成的梯度消失的问题，能够处理更长的上下文依赖</p>
<h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>attention 使用一种更加直接的方式处理当前词与上下文的关系，相比于LSTM, attention避免了梯度在序列间的传到，可以并行的计算，也能够处理更长的上下文依赖。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/26/序列标注进化史/" data-id="cjzsj8lxd00002l4u62g6ogr9" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-短文本的几点思考" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/26/短文本的几点思考/" class="article-date">
  <time datetime="2019-08-26T13:59:27.000Z" itemprop="datePublished">2019-08-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/26/短文本的几点思考/">短文本的几点思考</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-为什么lda在短文本上效果不好？"><a href="#1-为什么lda在短文本上效果不好？" class="headerlink" title="1. 为什么lda在短文本上效果不好？"></a>1. 为什么lda在短文本上效果不好？</h2><p>LDA是生成模型的一种，生成模型的参数分布都是从观测值中统计而来，短文本，也就意味着观测变量少，观测变量少，自然统计出来的概率分布不够准确。</p>
<h2 id="2-为什么短文本做分类经常效果不好？"><a href="#2-为什么短文本做分类经常效果不好？" class="headerlink" title="2. 为什么短文本做分类经常效果不好？"></a>2. 为什么短文本做分类经常效果不好？</h2><p>在文本分类中，真正起关键作用的往往是那一两个关键词，而短文本中往往缺乏相应的一些关键词，以及足够的上下文信息造成分类效果欠佳</p>
<h2 id="3-如何提升短文本上的分类效果？"><a href="#3-如何提升短文本上的分类效果？" class="headerlink" title="3. 如何提升短文本上的分类效果？"></a>3. 如何提升短文本上的分类效果？</h2><ul>
<li><p>特征扩展：<br>使用外部的一些知识扩展特征，比如使用在wiki上训练的LDA模型向量化当前的短文本，然后将此向量加入到文本分类的特征中，也可以使用一些attention机制去扩展外部的一些特征</p>
</li>
<li><p>标签传播<br>利用搜索引擎如es，在有标签数据中，找最相近的满足条件的数据，做标签的简单voting，或加入权重做voting。</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/26/短文本的几点思考/" data-id="cjzsj8lxl00022l4u7u7dqly5" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-deepsequence-最完整的深度学习序列标注工具" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/13/deepsequence-最完整的深度学习序列标注工具/" class="article-date">
  <time datetime="2019-08-13T09:55:53.000Z" itemprop="datePublished">2019-08-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/13/deepsequence-最完整的深度学习序列标注工具/">deepsequence-最完整的深度学习序列标注工具</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>deepsequence 是我在工作学习过程中写的一个做序列标注的工具，基于keras,支持各种经典的序列标注任务，如分词，NER(命名实体识别), 为什么是最完整的呢？</p>
<ul>
<li>支持BERT+CRF 和 BILSTM+CRF两种最优秀的序列标注的算法架构</li>
<li>支持字符级别的特征，charCNN 或者 charLSTM</li>
<li>支持postag特征的输入</li>
<li>支持领域知识的输入，如用于ner识别的公司字典</li>
<li>支持BIO, BIOLU, BIOES三种标注格式</li>
<li>支持通过配置文件快速定制模型细节及参数</li>
</ul>
<h2 id="架构解析"><a href="#架构解析" class="headerlink" title="架构解析"></a>架构解析</h2><h3 id="BILSTM-CRF"><a href="#BILSTM-CRF" class="headerlink" title="BILSTM + CRF"></a>BILSTM + CRF</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line">def build_bilstm(self, verbose=True):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    build model architecture from parameters</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    word_ids = Input(batch_shape=(None, None), dtype=&apos;int32&apos;, name=&apos;word_input&apos;)</span><br><span class="line">    inputs = [word_ids]</span><br><span class="line"></span><br><span class="line">    if self._params.use_pretrain_embedding:</span><br><span class="line">        if verbose: logging.info(&quot;initial word embedding with pretrained embeddings&quot;)</span><br><span class="line">        if self._params.word_embedding_dim == 100:</span><br><span class="line">            glove_file = self._params.data_dir + &apos;/glove.6B.100d.txt&apos;</span><br><span class="line">        elif self._params.word_embedding_dim == 300:</span><br><span class="line">            glove_file = self._params.data_dir + &apos;/glove.42B.300d.txt&apos;</span><br><span class="line">        else:</span><br><span class="line">            logging.error(&quot;we only support glove embedding with dimension 100 or 300&quot;)</span><br><span class="line">            raise ValueError(&quot;unmatch word dimension, we only support glove embedding with dimension 100 or 300&quot;)</span><br><span class="line">        glove_embedding_index = load_glove(glove_file, self._params.word_embedding_dim)</span><br><span class="line">        word_vocab = self.input_processor.word_vocab.vocab</span><br><span class="line">        glove_embeddings_matrix = np.zeros([len(word_vocab), self._params.word_embedding_dim])</span><br><span class="line">        for word, i in word_vocab.items():</span><br><span class="line">            vector = glove_embedding_index.get(word)</span><br><span class="line">            if vector is not None:</span><br><span class="line">                glove_embeddings_matrix[i] = vector</span><br><span class="line">        </span><br><span class="line">        word_embeddings = Embedding(input_dim=glove_embeddings_matrix.shape[0],</span><br><span class="line">                                    output_dim=glove_embeddings_matrix.shape[1],</span><br><span class="line">                                    trainable=False,</span><br><span class="line">                                    mask_zero=True,</span><br><span class="line">                                    weights=[glove_embeddings_matrix],</span><br><span class="line">                                    name=&apos;word_embedding&apos;)(word_ids)</span><br><span class="line">    else:</span><br><span class="line">        word_embeddings = Embedding(input_dim=self._params.word_vocab_size,</span><br><span class="line">                                    output_dim=self._params.word_embedding_dim,</span><br><span class="line">                                    mask_zero=True,</span><br><span class="line">                                    name=&apos;word_embedding&apos;)(word_ids)</span><br><span class="line"></span><br><span class="line">    input_embeddings = [word_embeddings]</span><br><span class="line">    if self._params.use_char:</span><br><span class="line">        char_ids = Input(batch_shape=(None, None, None), dtype=&apos;int32&apos;, name=&apos;char_input&apos;)</span><br><span class="line">        inputs.append(char_ids)</span><br><span class="line">        if self._params.char_feature == &quot;lstm&quot;:</span><br><span class="line">            char_embeddings = Embedding(input_dim=self._params.char_vocab_size,</span><br><span class="line">                                        output_dim=self._params.char_embedding_dim,</span><br><span class="line">                                        mask_zero=True,</span><br><span class="line">                                        name=&apos;char_embedding&apos;)(char_ids)</span><br><span class="line">            if verbose: logging.info(&quot;using charcter level lstm features&quot;)</span><br><span class="line">            char_feas = TimeDistributed(Bidirectional(LSTM(self._params.char_lstm_size)), name=&quot;char_lstm&quot;)(char_embeddings)</span><br><span class="line">        elif self._params.char_feature == &quot;cnn&quot;:</span><br><span class="line">            # cnn do not support mask</span><br><span class="line">            char_embeddings = Embedding(input_dim=self._params.char_vocab_size,</span><br><span class="line">                                        output_dim=self._params.char_embedding_dim,</span><br><span class="line">                                        name=&apos;char_embedding&apos;)(char_ids)</span><br><span class="line">            if verbose: logging.info(&quot;using charcter level cnn features&quot;)</span><br><span class="line">            char_feas = char_cnn_encode(char_embeddings, self._params.n_gram_filter_sizes, self._params.n_gram_filter_nums)</span><br><span class="line">        else:</span><br><span class="line">            raise ValueError(&apos;char feature must be lstm or cnn&apos;)</span><br><span class="line"></span><br><span class="line">        input_embeddings.append(char_feas)</span><br><span class="line"></span><br><span class="line">    if self._params.use_pos:</span><br><span class="line">        if verbose: logging.info(&quot;use pos tag features&quot;)</span><br><span class="line">        pos_ids = Input(batch_shape=(None, None), dtype=&apos;int32&apos;, name=&apos;pos_input&apos;)</span><br><span class="line">        inputs.append(pos_ids)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        pos_embeddings = Embedding(input_dim=self._params.pos_vocab_size,</span><br><span class="line">                                    output_dim=self._params.pos_embedding_dim,</span><br><span class="line">                                    mask_zero=True,</span><br><span class="line">                                    name=&apos;pos_embedding&apos;)(pos_ids)</span><br><span class="line">        input_embeddings.append(pos_embeddings)</span><br><span class="line"></span><br><span class="line">    if self._params.use_dict:</span><br><span class="line">        if verbose: logging.info(&quot;use user dict features&quot;)</span><br><span class="line">        dict_ids = Input(batch_shape=(None, None), dtype=&apos;int32&apos;, name=&apos;dict_input&apos;)</span><br><span class="line">        inputs.append(dict_ids)</span><br><span class="line"></span><br><span class="line">        dict_embeddings = Embedding(input_dim=self._params.dict_vocab_size,</span><br><span class="line">                                    output_dim=self._params.dict_embedding_dim,</span><br><span class="line">                                    mask_zero=True,</span><br><span class="line">                                    name=&apos;dict_embedding&apos;)(dict_ids)</span><br><span class="line">        input_embeddings.append(dict_embeddings)</span><br><span class="line"></span><br><span class="line">    input_embedding = Concatenate(name=&quot;input_embedding&quot;)(input_embeddings) if len(input_embeddings)&gt;1 else input_embeddings[0]</span><br><span class="line">    input_embedding_ln = LayerNormalization(name=&apos;input_layer_normalization&apos;)(input_embedding)</span><br><span class="line">    #input_embedding_bn = BatchNormalization()(input_embedding_ln)</span><br><span class="line">    input_embedding_drop = Dropout(self._params.dropout, name=&quot;input_embedding_dropout&quot;)(input_embedding_ln)</span><br><span class="line"></span><br><span class="line">    z = Bidirectional(LSTM(units=self._params.main_lstm_size, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),</span><br><span class="line">                       name=&quot;main_bilstm&quot;)(input_embedding_drop)</span><br><span class="line">    z = Dense(self._params.fc_dim, activation=&apos;tanh&apos;, name=&quot;fc_dense&quot;)(z)</span><br><span class="line"></span><br><span class="line">    if self._params.use_crf:</span><br><span class="line">        if verbose: logging.info(&apos;use crf decode layer&apos;)</span><br><span class="line">        crf = CRF(self._params.num_labels, sparse_target=False,</span><br><span class="line">                    learn_mode=&apos;marginal&apos;, test_mode=&apos;marginal&apos;, name=&apos;crf_out&apos;)</span><br><span class="line">        loss = crf.loss_function</span><br><span class="line">        pred = crf(z)</span><br><span class="line">    else:</span><br><span class="line">        loss = &apos;categorical_crossentropy&apos;</span><br><span class="line">        pred = Dense(self._params.num_labels, activation=&apos;softmax&apos;, name=&apos;softmax_out&apos;)(z)</span><br><span class="line"></span><br><span class="line">    model = Model(inputs=inputs, outputs=pred)</span><br><span class="line">    model.summary(print_fn=lambda x: logging.info(x + &apos;\n&apos;))</span><br><span class="line">    model.compile(loss=loss, optimizer=self._params.optimizer)</span><br><span class="line"></span><br><span class="line">    self.model = model</span><br></pre></td></tr></table></figure>

<h3 id="BERT-CRF"><a href="#BERT-CRF" class="headerlink" title="BERT + CRF"></a>BERT + CRF</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">def build_bert(self, verbose=True):</span><br><span class="line"></span><br><span class="line">    bert_word_ids = Input(batch_shape=(None, None), dtype=&quot;int32&quot;, name=&quot;bert_word_input&quot;)</span><br><span class="line">    bert_mask_ids = Input(batch_shape=(None, None), dtype=&quot;int32&quot;, name=&apos;bert_mask_input&apos;)</span><br><span class="line">    bert_segment_ids = Input(batch_shape=(None, None), dtype=&quot;int32&quot;, name=&quot;bert_segment_input&quot;)</span><br><span class="line">    </span><br><span class="line">    inputs = [bert_word_ids, bert_mask_ids, bert_segment_ids]</span><br><span class="line"></span><br><span class="line">    bert_out = BertLayer(n_fine_tune_layers=self._params.n_fine_tune_layers, bert_path=self._params.bert_path, name=&quot;bert_layer&quot;)([bert_word_ids, bert_mask_ids, bert_segment_ids])</span><br><span class="line"></span><br><span class="line">    features = bert_out</span><br><span class="line"></span><br><span class="line">    if self._params.use_dict:</span><br><span class="line">        if verbose: logging.info(&quot;use user dict features&quot;)</span><br><span class="line">        dict_ids = Input(batch_shape=(None, None), dtype=&apos;int32&apos;, name=&apos;dict_input&apos;)</span><br><span class="line">        inputs.append(dict_ids)</span><br><span class="line"></span><br><span class="line">        dict_embeddings = Embedding(input_dim=self._params.dict_vocab_size,</span><br><span class="line">                                    output_dim=self._params.dict_embedding_dim,</span><br><span class="line">                                    mask_zero=True,</span><br><span class="line">                                    name=&apos;dict_embedding&apos;)(dict_ids)</span><br><span class="line"></span><br><span class="line">        features = Concatenate(name=&quot;bert_and_dict_features&quot;)([features, dict_embeddings])</span><br><span class="line"></span><br><span class="line">    z = Dense(self._params.fc_dim, activation=&apos;relu&apos;, name=&quot;fc_dense&quot;)(features)</span><br><span class="line"></span><br><span class="line">    if self._params.use_crf:</span><br><span class="line">        if verbose: logging.info(&apos;use crf decode layer&apos;)</span><br><span class="line">        crf = CRF(self._params.num_labels, sparse_target=False,</span><br><span class="line">                    learn_mode=&apos;marginal&apos;, test_mode=&apos;marginal&apos;, name=&apos;crf_out&apos;)</span><br><span class="line">        loss = crf.loss_function</span><br><span class="line">        pred = crf(z)</span><br><span class="line">    else:</span><br><span class="line">        loss = &apos;categorical_crossentropy&apos;</span><br><span class="line">        pred = Dense(self._params.num_labels, activation=&apos;softmax&apos;, name=&apos;softmax_out&apos;)(z)</span><br><span class="line"></span><br><span class="line">    model = Model(inputs=inputs, outputs=pred)</span><br><span class="line">    model.summary(print_fn=lambda x: logging.info(x + &apos;\n&apos;))</span><br><span class="line"></span><br><span class="line">    # It is recommended that you use this optimizer for fine tuning, since this</span><br><span class="line">    # is how the model was trained (note that the Adam m/v variables are NOT</span><br><span class="line">    # loaded from init_checkpoint.)</span><br><span class="line">    optimizer = AdamWeightDecayOptimizer(</span><br><span class="line">        learning_rate=1e-5,</span><br><span class="line">        weight_decay_rate=0.01,</span><br><span class="line">        beta_1=0.9,</span><br><span class="line">        beta_2=0.999,</span><br><span class="line">        epsilon=1e-6,</span><br><span class="line">        exclude_from_weight_decay=[&quot;LayerNorm&quot;, &quot;layer_norm&quot;, &quot;bias&quot;])</span><br><span class="line">    </span><br><span class="line">    model.compile(loss=loss, optimizer=optimizer)</span><br><span class="line"></span><br><span class="line">    self.model = model</span><br></pre></td></tr></table></figure>

<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>训练NER模型：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">import argparse</span><br><span class="line">import logging</span><br><span class="line"></span><br><span class="line">from deepsequence.dataset import load_conll</span><br><span class="line">from deepsequence.model import SequenceModel</span><br><span class="line">from deepsequence.config import Params</span><br><span class="line">from deepsequence.utils import set_logger</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main(params):</span><br><span class="line"></span><br><span class="line">    train_file = params.data_dir + &apos;/train.txt&apos;</span><br><span class="line">    valid_file = params.data_dir + &apos;/valid.txt&apos;</span><br><span class="line">    train_data = load_conll(train_file, params)</span><br><span class="line">    valid_data = load_conll(valid_file, params)</span><br><span class="line"></span><br><span class="line">    model = SequenceModel(params)</span><br><span class="line"></span><br><span class="line">    model_file = params.data_dir + &apos;/model/model.h5&apos;</span><br><span class="line">    if params.continue_previous_training is True:</span><br><span class="line"></span><br><span class="line">        logging.info(&quot;restore model from local&quot;)</span><br><span class="line">        model.restore(model_file)</span><br><span class="line">    else:</span><br><span class="line">        logging.info(&quot;model initializing...&quot;)</span><br><span class="line">        model.build(params)</span><br><span class="line"></span><br><span class="line">    model.fit(train_data, valid_data, verbose=True)</span><br><span class="line">    model.evaluate(valid_data)</span><br><span class="line">    logging.info(&quot;model save to &#123;&#125;&quot;.format(model_file))</span><br><span class="line">    model.save(model_file)</span><br><span class="line"></span><br><span class="line">    tf_saved_model_dir = params.data_dir + &apos;/model/tf_saved_model&apos;</span><br><span class="line">    model.export_sm(tf_saved_model_dir)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line"></span><br><span class="line">    parser = argparse.ArgumentParser(description=&apos;&apos;&apos;train deep sequence model&apos;&apos;&apos;)</span><br><span class="line">    parser.add_argument(&apos;--config&apos;, required=True)</span><br><span class="line"></span><br><span class="line">    set_logger(&apos;train.log&apos;)</span><br><span class="line"></span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    logging.info(&quot;parse config file path: &#123;&#125;&quot;.format(args.config))</span><br><span class="line"></span><br><span class="line">    params = Params(args.config)</span><br><span class="line">    logging.info(&quot;parameters: &#123;&#125;&quot;.format(params.dict))</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        main(params)</span><br><span class="line">    except Exception as e:</span><br><span class="line">        logging.error(&apos;run fail&apos;, exc_info=True)</span><br></pre></td></tr></table></figure>

<p>配置文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;model&quot;: &quot;bilstm&quot;,</span><br><span class="line">    </span><br><span class="line">    &quot;vocab_pad&quot;: &quot;PAD&quot;,</span><br><span class="line">    &quot;vocab_unk&quot;: &quot;UNK&quot;,</span><br><span class="line">    &quot;max_sent_len&quot;: 1000,</span><br><span class="line">    &quot;max_word_len&quot;: 24,</span><br><span class="line"></span><br><span class="line">    &quot;word_embedding_dim&quot;: 100,</span><br><span class="line">    &quot;use_pretrain_embedding&quot;: true, </span><br><span class="line"></span><br><span class="line">    &quot;use_char&quot;: true,</span><br><span class="line">    &quot;char_embedding_dim&quot;: 30, </span><br><span class="line">    &quot;char_feature&quot;: &quot;cnn&quot;,</span><br><span class="line">    &quot;n_gram_filter_sizes&quot;: [1, 2, 3, 4, 5],</span><br><span class="line">    &quot;n_gram_filter_nums&quot;: [5, 10, 20, 30, 35],</span><br><span class="line">    &quot;char_lstm_size&quot;: 50,</span><br><span class="line"></span><br><span class="line">    &quot;use_pos&quot;: true, </span><br><span class="line">    &quot;conll_pos_index&quot;: 1,</span><br><span class="line">    &quot;pos_embedding_dim&quot;: 30,</span><br><span class="line"></span><br><span class="line">    &quot;use_dict&quot;: true, </span><br><span class="line">    &quot;dict_embedding_dim&quot;: 10,</span><br><span class="line">     </span><br><span class="line">    &quot;dropout&quot;: 0.5, </span><br><span class="line">     </span><br><span class="line">    &quot;main_lstm_size&quot;: 100, </span><br><span class="line">    &quot;fc_dim&quot;: 100, </span><br><span class="line">    &quot;use_crf&quot;: true, </span><br><span class="line">    &quot;optimizer&quot;: &quot;adam&quot;, </span><br><span class="line"></span><br><span class="line">    &quot;batch_size&quot;: 50, </span><br><span class="line">    &quot;max_train_epoch&quot;: 80, </span><br><span class="line">    </span><br><span class="line">    &quot;early_stop&quot;: 80, </span><br><span class="line">    </span><br><span class="line">    &quot;continue_previous_training&quot;: false,</span><br><span class="line">    &quot;data_dir&quot;: &quot;/xxxx/deepsequence/examples/data&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>更多细节可以可以在项目的GitHub页面查看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/yangdc1992/deepsequence</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/13/deepsequence-最完整的深度学习序列标注工具/" data-id="cjzsj8lxj00012l4uwpnuxd3u" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-主动学习" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/13/主动学习/" class="article-date">
  <time datetime="2019-08-13T09:55:07.000Z" itemprop="datePublished">2019-08-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/13/主动学习/">主动学习</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>主动学习是非常有用的模型迭代的策略和思想，具体概念大家可以自行搜索。这里主要介绍下主动学习关注的两个非常重要的点：</p>
<ul>
<li>数据的多样性，即数据集要尽量覆盖不同类型的数据。</li>
<li>模型结果的不确定性，不确定即指模型对于给出的结果是不确定的，这里最直观的体现就是模型输出的概率，当模型输出一个结果，但相应的概率却较低的时候，就表明我们的模型对这一类case的学习泛化能力还不够。也是我们在模型迭代中要重点关注的对象。</li>
</ul>
<p>基于如上两个原则，主动学习能够很好的帮助我们了解数据，了解模型，从而指导我们调整数据分布，调整模型，完成模型的迭代。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/13/主动学习/" data-id="cjzsj8lxn00032l4uaniz30lw" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-小数据VS大模型" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/13/小数据VS大模型/" class="article-date">
  <time datetime="2019-08-13T09:54:33.000Z" itemprop="datePublished">2019-08-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/13/小数据VS大模型/">小数据VS大模型</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>首先，我要先说下实际工作中我们容易遇到的两个现状或痛点：</p>
<ul>
<li>像bert，xnet之类的state of the art 模型架构越来越复杂，模型的容量越来越大</li>
<li>实际工作中，获取训练数据的成本高，实际可用的训练数据量小</li>
</ul>
<p>那直接把小数据喂给大模型会产生什么后果呢？</p>
<ul>
<li>过拟合，当数据量少，模型容量大的时候，最直接的后果就是过拟合，模型学到的可能只是一些非常肤浅的特征，或只是记住了当前数据集里的一些简单模式，在遇到新的数据时，泛化性能差。</li>
</ul>
<p>相应的解决方案：</p>
<ul>
<li>数据增强，可以使用同义词替换等方式对原数据加入更多噪声，避免让模型学到的只是一些肤浅的模式</li>
<li>使用预训练的词向量或模型，预训练的词向量或模型本身带有大量的信息，可以降低整体模型的学习难度</li>
<li>加入更多领域知识，为避免模型学习到的只是一些肤浅的简单模式，可以在输入特征中加入更多的领域知识，指导模型学习</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/13/小数据VS大模型/" data-id="cjzsj8lxp00042l4unlevqivi" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-数据不平衡的几点思考" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/13/数据不平衡的几点思考/" class="article-date">
  <time datetime="2019-08-13T09:54:13.000Z" itemprop="datePublished">2019-08-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/13/数据不平衡的几点思考/">数据不平衡的几点思考</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-不平衡并不仅仅只是某一类的数据少，数据冗余也是种不平衡"><a href="#1-不平衡并不仅仅只是某一类的数据少，数据冗余也是种不平衡" class="headerlink" title="1. 不平衡并不仅仅只是某一类的数据少，数据冗余也是种不平衡"></a>1. 不平衡并不仅仅只是某一类的数据少，数据冗余也是种不平衡</h2><h3 id="什么是数据冗余？"><a href="#什么是数据冗余？" class="headerlink" title="什么是数据冗余？"></a>什么是数据冗余？</h3><p>通俗来讲，数据冗余，就是说某一类case，对于学习模型来说，我只需要100个训练样本，就可以学到这类case，但训练数据里却有远超过100个数量的这一类case</p>
<h3 id="数据冗余的问题？"><a href="#数据冗余的问题？" class="headerlink" title="数据冗余的问题？"></a>数据冗余的问题？</h3><ul>
<li>浪费标注人员的人力，物力</li>
<li>挤占其他类别的学习机会，在计算学习器损失的时候这一类样本的损失占了很大比例，导致其他类别的损失占比小，降低模型收敛速度和效果</li>
</ul>
<h2 id="2-脱离数据分布谈评估指标都是耍流氓"><a href="#2-脱离数据分布谈评估指标都是耍流氓" class="headerlink" title="2. 脱离数据分布谈评估指标都是耍流氓"></a>2. 脱离数据分布谈评估指标都是耍流氓</h2><p>在工作中，我们经常遇到的一个情况便是模型在训练测试集上评价指标非常好，但拿一些实际业务数据测的时候却发现效果测试集上评测指标显示的那么理想，为什么？</p>
<ul>
<li>测试数据分布不合理，当训练数据中，某一类简单的case占了大部分的时候，默认的评价指标自然会比较好看，但这样其实掩盖了真正问题，那些数据较少，又对实际业务影响较大的case，效果如何，很难从默认的评价指标中看出来</li>
</ul>
<h2 id="3-什么才是好的训练数据的分布？绝对平衡吗？"><a href="#3-什么才是好的训练数据的分布？绝对平衡吗？" class="headerlink" title="3. 什么才是好的训练数据的分布？绝对平衡吗？"></a>3. 什么才是好的训练数据的分布？绝对平衡吗？</h2><p>说到数据不平衡问题，大家首先想到的是通过降采样或上采样或其他数据增强的方式把不同类的数据变得<strong>数量上的绝对平衡</strong>，但绝对平衡一定是好的吗？个人认为，好的训练数据分布应该是<strong>在不违背总体先验分布的情况下，与各个类别的学习难度成正比</strong>，如果某一类别比较复杂，模型比较难学，那就多增加一些这类的样本，如果某一类别比较简单，就少增加一些这类的样本</p>
<h2 id="4-如何在实际业务中准备分布合理的训练数据？"><a href="#4-如何在实际业务中准备分布合理的训练数据？" class="headerlink" title="4. 如何在实际业务中准备分布合理的训练数据？"></a>4. 如何在实际业务中准备分布合理的训练数据？</h2><p>模型的发展是一个迭代的过程，没有模型能够在一开始就很好的适应各种业务问题，相对应的训练数据也是一个迭代的过程，在迭代的过程中让分布越来越合理，实际工作中，在每一轮模型迭代的过程中，我们会通过模型输出的概率等方式去看模型在实际业务数据中的表现，通过这种方式指导调整数据的分布，以及模型的架构，最后完成这一轮的迭代。</p>
<h2 id="5-处理数据不平衡的方法总结"><a href="#5-处理数据不平衡的方法总结" class="headerlink" title="5. 处理数据不平衡的方法总结"></a>5. 处理数据不平衡的方法总结</h2><ul>
<li>调整数据分布</li>
<li>使用非参数模型，如svm，基于决策树的模型（决策树，随机森林，gbdt…）, svm基于最大间隔优化，决策树基于最大信息增益优化，他们的优化方式决定了相比逻辑回归等参数权值优化的方式，天然对数据不平衡问题更加鲁棒</li>
<li>在计算损失时，对样本加权，对数量少的类别使用更大的样本权重，使得模型在优化时更加关注这一类的样本</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/13/数据不平衡的几点思考/" data-id="cjzsj8lxq00052l4upmhepqm1" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/08/26/序列标注进化史/">序列标注进化史</a>
          </li>
        
          <li>
            <a href="/2019/08/26/短文本的几点思考/">短文本的几点思考</a>
          </li>
        
          <li>
            <a href="/2019/08/13/deepsequence-最完整的深度学习序列标注工具/">deepsequence-最完整的深度学习序列标注工具</a>
          </li>
        
          <li>
            <a href="/2019/08/13/主动学习/">主动学习</a>
          </li>
        
          <li>
            <a href="/2019/08/13/小数据VS大模型/">小数据VS大模型</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 杨德城/Decheng Yang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>